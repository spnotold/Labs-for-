id,instruction,context,dolly_category,tag,tag_reason
1,"Given this paragraph about Arlington, Virginia, what are the 3 public high schools in Arlington?","Arlington Public Schools operates the county's public K-12 education system of 22 elementary schools; 6 middle schools (Dorothy Hamm Middle School, Gunston Middle School, Kenmore Middle School, Swanson Middle School, Thomas Jefferson Middle School, and Williamsburg Middle School); and 3 public high schools (Wakefield High School, Washington-Liberty High School, and Yorktown High School). H-B Woodlawn and Arlington Tech are alternative public schools. Arlington County spends about half of its local revenues on education. For the FY2013 budget, 83 percent of funding was from local revenues, and 12 percent from the state. Per pupil expenditures are expected to average $18,700, well above its neighbors, Fairfax County ($13,600) and Montgomery County ($14,900).",,closeqa,问题询问明确具体的事实信息，答案可在提供的文本中直接找到
2,"Given these paragraphs about Large language models, when did LLMs emerge?","A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabelled text using self-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.

Properties
Though the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.

Though trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate considerable general knowledge about the world, and are able to ""memorize"" a great quantity of facts during training.

Hallucinations
Main article: Hallucination (artificial intelligence)
In artificial intelligence in general, and in large language models in particular, a ""hallucination"" is a confident response that does not seem to be justified by the model's training data.

Emergent abilities

On a number of natural language benchmarks involving tasks such as question answering, models perform no better than random chance until they reach a certain scale (in this case, measured by training computation), at which point their performance sharply increases. These are examples of emergent abilities.
Unpredictable abilities that have been observed in large language models but that were not present in simpler models (and that were not explicitly designed into the model) are usually called ""emergent abilities"". Researchers note that such abilities ""cannot be predicted simply by extrapolating the performance of smaller models"". These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed. Hundreds of emergent abilities have been described. Examples include multi-step arithmetic, taking college-level exams, identifying the intended meaning of a word, chain-of-thought prompting, decoding the International Phonetic Alphabet, unscrambling a word’s letters, identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.

Architecture and training
Large language models have most commonly used the transformer architecture, which, since 2018, has become the standard deep learning technique for sequential data (previously, recurrent architectures such as the LSTM were most common). LLMs are trained in an unsupervised manner on unannotated text. A left-to-right transformer is trained to maximize the probability assigned to the next word in the training data, given the previous context. Alternatively, an LLM may use a bidirectional transformer (as in the example of BERT), which assigns a probability distribution over words given access to both preceding and following context. In addition to the task of predicting the next word or ""filling in the blanks"", LLMs may be trained on auxiliary tasks which test their understanding of the data distribution such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear side-by-side in the training corpus.

The earliest LLMs were trained on corpora having on the order of billions of words. The first model in OpenAI's GPT series was trained in 2018 on BookCorpus, consisting of 985 million words. In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words. In the years since then, training corpora for LLMs have increased by orders of magnitude, reaching up to hundreds of billions or trillions of tokens.

LLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (1-2 orders of magnitude smaller than the state of the art at the time) at $1.6 million.

A 2020 analysis found that neural language models' capability (as measured by training loss) increased smoothly in a power law relationship with number of parameters, quantity of training data, and computation used for training. These relationships were tested over a wide range of values (up to seven orders of magnitude) and no attenuation of the relationship was observed at the highest end of the range (including for network sizes up to trillions of parameters).

Application to downstream tasks
Between 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via ""prompting"" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.

Fine-tuning
Main article: Fine-tuning (machine learning)
Fine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be ""frozen"", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).

Prompting
See also: Prompt engineering and Few-shot learning (natural language processing)
In the prompting paradigm, popularized by GPT-3, the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In ""few-shot prompting"", the prompt includes a small number of examples of similar (problem, solution) pairs. For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:

Review: This movie stinks.
Sentiment: negative

Review: This movie is fantastic!
Sentiment:

If the model outputs ""positive"", then it has correctly solved the task. In zero-shot prompting, no solve examples are provided. An example of a zero-shot prompt for the same sentiment analysis task would be ""The sentiment associated with the movie review 'This movie is fantastic!' is"".

Few-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence. The creation and optimisation of such prompts is called prompt engineering.

Instruction tuning
Instruction tuning is a form of fine-tuning designed to facilitate more natural and accurate zero-shot prompting interactions. Given a text input, a pretrained language model will generate a completion which matches the distribution of text on which it was trained. A naive language model given the prompt ""Write an essay about the main themes of Hamlet."" might provide a completion such as ""A late penalty of 10% per day will be applied to submissions received after March 17."" In instruction tuning, the language model is trained on many examples of tasks formulated as natural language instructions, along with appropriate responses. Various techniques for instruction tuning have been applied in practice. OpenAI's InstructGPT protocol involves supervised fine-tuning on a dataset of human-generated (prompt, response) pairs, followed by reinforcement learning from human feedback (RLHF), in which a reward function was learned based on a dataset of human preferences. Another technique, ""self-instruct"", fine-tunes the language model on a training set of examples which are themselves generated by an LLM (bootstrapped from a small initial set of human-generated examples).

https://en.wikipedia.org/wiki/Large_language_model",,closeqa,问题明确询问LLMs何时出现，答案在文本中直接提到为2018年，属于有明确答案的封闭式问答。
3,"Given this paragraph about ferrets, do they spend the majority of the time sleeping in a 24 hour day?","Ferrets spend 14–18 hours a day asleep and are most active around the hours of dawn and dusk, meaning they are crepuscular. If they are caged, they should be taken out daily to exercise and satisfy their curiosity; they need at least an hour and a place to play. Unlike their polecat ancestors, which are solitary animals, most ferrets will live happily in social groups. They are territorial, like to burrow, and prefer to sleep in an enclosed area.",,closeqa,问题询问雪貂是否在24小时内大部分时间睡觉，答案可通过文中明确提供的14-18小时睡眠时间直接判断。
4,Who does the Navy Cross is awarded to?,"The Navy Cross is the United States Naval Service's second-highest military decoration awarded for sailors and marines who distinguish themselves for extraordinary heroism in combat with an armed enemy force. The medal is equivalent to the Army's Distinguished Service Cross, the Air and Space Forces' Air Force Cross, and the Coast Guard Cross.

The Navy Cross is bestowed by the Secretary of the Navy and may also be awarded to members of the other armed services, and to foreign military personnel while serving with the U.S. Naval Service. The Navy Cross was established by Act of Congress (Public Law 65-253) and approved on February 4, 1919.",,closeqa,问题询问特定奖项授予对象，答案可从材料中直接提取，具有明确单一性。
5,"Given a reference text about The Universal Data Element Framework (UDEF), tell me how the framework is used.","The Universal Data Element Framework (UDEF) was  a controlled vocabulary developed by The Open Group. It provided a framework for categorizing, naming, and indexing data. It assigned to every item of data a structured alphanumeric tag plus a controlled vocabulary name that describes the meaning of the data. This allowed relating data elements to similar elements defined by other organizations.

UDEF defined a Dewey-decimal like code for each concept. For example, an ""employee number"" is often used in human resource management. It has a UDEF tag a.5_12.35.8 and a controlled vocabulary description ""Employee.PERSON_Employer.Assigned.IDENTIFIER"".

UDEF has been superseded by the Open Data Element Framework (O-DEF).",,open_qa,提问是关于UDEF框架如何被使用的，答案需要解释其工作机制，答案开放且需基于提供的文本进行综合描述。
6,What was Dick Lammi's most famous work?,"Dick Lammi (January 15, 1909 – November 29, 1969) was an American jazz tubist and bassist associated with Dixieland jazz. Lammi played violin and banjo early in his career, and played as a banjoist in various groups in the Pacific Northwest in the late 1920s. He settled in Portland, Oregon in the early 1930s, and played bass in a group there; after a move to San Francisco in 1936, he began playing tuba alongside bass. His best-known work was as a member of Lu Watters's band, the Yerba Buena Jazz Band. Lammi played in the ensemble from 1941 to 1950, including on virtually all of their recordings.",,open_qa,问题询问人物最著名的作品，需基于提供的百科信息推断答案
7,Wat is goede vrijdag?,"Goede Vrijdag is de vrijdag voor Pasen. Op deze dag herdenken christenen de kruisiging en dood van Jezus. Jezus werd volgens de Bijbel veroordeeld tot de kruisdood door de Romeinse stadhouder Pontius Pilatus, op aandrang van het sanhedrin. Deze straf werd voltrokken op de heuvel Golgotha nabij de stad Jeruzalem. Goede Vrijdag volgt op Witte Donderdag en gaat vooraf aan Stille Zaterdag. Daarop volgt Pasen.",,closeqa,Vraag naar specifieke betekenis van Goede Vrijdag met eenduidig antwoord uit religieuze context.
8,"Given this text about Major League Baseball, how many games do teams play in the regular season and how many games are played in the World Series.","Major League Baseball (MLB) is a professional baseball organization and the oldest major professional sports league in the world.[B] MLB is composed of 30 teams, divided equally between the National League (NL) and the American League (AL), with 29 in the United States and 1 in Canada. Formed in 1876 and 1901 respectively, the NL and AL cemented their cooperation with the National Agreement in 1903. They remained legally separate entities until 2000, when they merged into a single organization led by the Commissioner of Baseball. MLB is headquartered in Midtown Manhattan. It is considered one of the major professional sports leagues in the United States and Canada. 

Each team plays 162 games per season, and six teams in each league advance to a four-round postseason tournament that culminates in the World Series, a best-of-seven championship series between the two league champions first played in 1903. The New York Yankees have the most championships: 27. The reigning champions are the Houston Astros, who defeated the Philadelphia Phillies, 4–2, in the 2022 World Series.",,closeqa,问题询问具体数字答案，答案可直接从文本中提取。
9,What is Douglas Stuart Moore's best remembered for?,"Douglas Stuart Moore (August 10, 1893 – July 25, 1969) was an American composer, songwriter, organist, pianist, conductor, educator, actor, and author. A composer who mainly wrote works with an American subject, his music is generally characterized by lyricism in a popular or conservative style which generally eschewed the more experimental progressive trends of musical modernism. Composer Virgil Thomson described Moore as a neoromantic composer who was influenced by American folk music. While several of his works enjoyed popularity during his lifetime, only his folk opera The Ballad of Baby Doe (1956) has remained well known into the 21st century.",,open_qa,提问关于人物的主要成就，答案需基于提供的背景信息综合判断
10,Which club has won the most number of UEFA Champions League?,"The UEFA Champions League is a seasonal football competition established in 1955. Prior to the 1992–93 season, the tournament was named the European Cup. The UEFA Champions League is open to the league champions of all UEFA (Union of European Football Associations) member associations (except Liechtenstein, which has no league competition), as well as to the clubs finishing from second to fourth position in the strongest leagues. Originally, only the champions of their respective national league and the defending champions of the competition were allowed to participate. However, this was changed in 1997 to allow the runners-up of the stronger leagues to compete as well, and again in 1999 when third and fourth-placed teams of the said leagues also became eligible. In the Champions League era, the defending champions of the competition did not automatically qualify until the rules were changed in 2005 to allow title holders Liverpool to enter the competition.

Teams that have won the UEFA Champions League three consecutive times, or five times overall, receive a multiple-winner badge. Six teams have earned this privilege: Real Madrid, Ajax, Bayern Munich, Milan, Liverpool, and Barcelona. Until 2009, clubs that had earned that badge were allowed to keep the European Champion Clubs' Cup and a new one was commissioned; since 2009, the winning team each year has received a full-size replica of the trophy, while the original is retained by UEFA.

A total of 22 clubs have won the Champions League/European Cup. Real Madrid hold the record for the most victories, having won the competition fourteen times, including the inaugural edition. They have also won the competition the most consecutive times, with five straight titles from 1956 to 1960. Juventus have been runners-up the most times, losing seven finals. Atlético Madrid is the only team to reach three finals without having won the trophy while Reims and Valencia have finished as runners-up twice without winning. Spain has provided the most champions, with nineteen wins from two clubs. England have produced fourteen winners from five clubs and Italy have produced twelve winners from three clubs. English teams were banned from the competition for five years following the Heysel disaster in 1985. The current champions are Real Madrid, who beat Liverpool 1–0 in the 2022 final for a record-extending fourteenth title.",,closeqa,问题询问哪个俱乐部赢得最多欧冠冠军，答案明确且唯一（Real Madrid），可直接从材料中提取。
